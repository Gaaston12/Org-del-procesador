<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Code: The Hidden Language of Computer Hardware and Software</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<div id="filepos1065365" style="height:0pt"></div><div id="filepos1065365" class="calibre_">
<div class="calibre_">
<div class="calibre_">
<div class="calibre_">
<h1 class="calibre1" id="calibre_pb_61"><span class="calibre2"><a shape="rect" class="calibre4"></a>Chapter 18. From Abaci to Chips </span></h1>
</div>
</div>
</div>
<p class="calibre_1">Throughout recorded history, people have invented numerous clever gadgets and machines in a universal quest to make mathematical calculations just a little bit easier. While the human species seemingly has an innate numerical ability, we also require frequent assistance. We can often conceive of problems that we can't easily solve ourselves. </p>
<p class="calibre_1">The development of number systems can be seen as an early tool to help people keep track of commodities and property. Many cultures, including the ancient Greeks and native Americans, seem to have counted with the assistance also of pebbles or kernels of grain. In Europe, this led to counting boards, and in the Middle East to the familiar frame-and-bead <a shape="rect"></a>abacus: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00375.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">Although commonly associated with Asian cultures, the abacus seems to have been introduced to China by traders around 1200 CE. </p>
<p class="calibre_1">No one has ever really enjoyed multiplication and division, but few people have done anything about it. The Scottish mathematician John <a shape="rect"></a>Napier (1550–1617) was one of those few. He invented logarithms for the specific purpose of simplifying these operations. The product of two numbers is simply the sum of their logarithms. So if you need to multiply two numbers, you look them up in a table of logarithms, add the numbers from the table, and then use the table in reverse to find the actual product. </p>
<p class="calibre_1">The construction of tables of logarithms occupied some of the greatest minds of the subsequent 400 years while others designed little gadgets to use in place of these tables. The slide rule has a long history beginning with a logarithmic scale made by Edmund <a shape="rect"></a>Gunter (1581–1626) and refined by William <a shape="rect"></a>Oughtred (1574–1660). The history of the slide rule effectively ended in 1976, when the <a shape="rect"></a>Keuffel &amp; Esser Company presented its last manufactured slide rule to the Smithsonian Institution in Washington D.C. The cause of death was the hand-held calculator. </p>
<p class="calibre_1"><a shape="rect"></a>Napier also invented another multiplication aid, which is composed of strips of numbers usually inscribed on bone, horn, or ivory and hence referred to as <span><em class="italic">Napier's Bones</em></span>. The earliest mechanical calculator was a somewhat automated version of Napier's bones built around 1620 by Wilhelm Schickard (1592–1635). Other <a shape="rect"></a>calculators based on interlocking wheels, gears, and levers are almost as old. Two of the more significant builders of mechanical calculators were the mathematicians and philosophers Blaise <a shape="rect"></a>Pascal (1623–1662) and Gottfried Wilhelm von <a shape="rect"></a>Leibniz (1646–1716).<a shape="rect"></a></p>
<p class="calibre_1">You'll no doubt recall what a nuisance the carry bit was in both the original 8-Bit Adder and the computer that (among other things) automated the addition of numbers wider than 8 bits. The carry seems at first to be just a little quirk of addition, but in adding machines, the carry is really the central problem. If you've designed an adding machine that does everything except the carry, you're nowhere close to being finished! </p>
<p class="calibre_1">How successfully the carry is dealt with is a key to the evaluation of old calculating machines. For example, Pascal's design of the carry mechanism prohibited the machine from subtracting. To subtract, the nines' complement had to be added the way that I demonstrated in <a shape="rect" href="index_split_016.html#filepos625949">Chapter 13</a>. Successful mechanical calculators that real people could use weren't available until the late nineteenth century. </p>
<p class="calibre_1">One curious invention that was to have a later influence on the history of computing—as well as a profound influence on the textile industry—was an automated loom developed by Joseph Marie <a shape="rect"></a>Jacquard (1752–1834). The Jacquard loom (circa 1801) used metal cards with holes punched in them (much like those of a player piano) to control the weaving of patterns in fabrics. Jacquard's own tour de force was a self-portrait in black and white silk that required about 10,000 cards. </p>
<p class="calibre_1">In the eighteenth century (and indeed up to the 1940s), a <span><em class="italic">computer</em></span> was a person who calculated numbers for hire. Tables of logarithms were always needed, and trigonometric tables were essential for nautical navigation using the stars and planets. If you wanted to publish a new set of tables, you would hire a bunch of computers, set them to work, and then assemble all the results. Errors could creep in at any stage of this process, of course, from the initial calculation to setting up the type to print the final pages. </p>
<p class="calibre_1">The desire to eliminate errors from mathematical tables motivated the work of <a shape="rect"></a>Charles Babbage (1791–1871), a British mathematician and economist who was almost an exact contemporary of Samuel <a shape="rect"></a>Morse. </p>
<p class="calibre_1">At the time, mathematical tables (of logarithms, for example) were <span><em class="italic">not</em></span> created by calculating an actual logarithm for each and every entry in the table. This would have taken far too long. Instead, the logarithms were calculated for select numbers, and then numbers in between were calculated by interpolation, using what are called <span><em class="italic">differences</em></span> in relatively simple calculations. </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00376.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">Beginning about 1820, <a shape="rect"></a>Babbage believed that he could design and build a machine that would automate the process of constructing a table, even to the point of setting up type for printing. This would eliminate errors. He conceived the <a shape="rect"></a>Difference Engine, and basically it was a big mechanical adding machine. Multidigit decimal numbers were represented by geared wheels that could be in any of 10 positions. Negatives were handled using the ten's complement. Despite some early models that showed Babbage's design to be sound and some grants from the British government (never enough, of course), the Difference Engine was never completed. Babbage abandoned work on it in 1833. </p>
<p class="calibre_1">By that time, however, Babbage had an even better idea. It was called the <a shape="rect"></a>Analytical Engine, and through repeated design and redesign (with a few small models and parts of it actually built) it consumed Babbage off and on until his death. The Analytical Engine is the closest thing to a computer that the nineteenth century has to offer. In Babbage's design, it had a <span><em class="italic">store</em></span> (comparable to our concept of memory) and a <span><em class="italic">mill</em></span> (the arithmetic unit). Multiplication could be handled by repeated addition, and division by repeated subtraction. </p>
<p class="calibre_1">What's most intriguing about the Analytical Engine is that it could be programmed using cards that were adapted from the cards used in the Jacquard pattern-weaving loom. As Augusta Ada <a shape="rect"></a>Byron, Countess of Lovelace (1815–1852), put it (in notes to her translation of an article written by an Italian mathematician about Babbage's Analytical Engine), "We may say that the Analytical Engine weaves algebraical patterns just as the Jacquard-loom weaves flowers and leaves." </p>
<p class="calibre_1">Babbage seems to be the first person to understand the importance of a conditional jump in computers. Here's Ada Byron again: "A <span><em class="italic">cycle</em></span> of operations, then, must be understood to signify any <span><em class="italic">set of operations</em></span> which is repeated <span><em class="italic">more than once</em></span>. It is equally a <span><em class="italic">cycle</em></span>, whether it be repeated <span><em class="italic">twice</em></span> only, or an indefinite number of times; for it is the fact of a <span><em class="italic">repetition occurring at all</em></span> that constitutes it such. In many cases of analysis there is a <span><em class="italic">recurring group</em></span> of one or more cycles; that is, a <span><em class="italic">cycle of cycle</em></span>, or a <span><em class="italic">cycle of cycles</em></span>." </p>
<p class="calibre_1">Although a difference engine was eventually built by father-and-son team Georg and Edvard <a shape="rect"></a>Scheutz in 1853, <a shape="rect"></a>Babbage's engines were forgotten for many years, only to be resurrected in the 1930s when people began searching for the roots of twentieth century computing. By that time, everything Babbage had done had already been surpassed by later technology, and he had little to offer the twentieth century computer engineer except a precocious vision of automation. </p>
<p class="calibre_1">Another milestone in the history of computing resulted from Article I, Section 2, of the <a shape="rect"></a>Constitution of the United States of America. Among other things, this section calls for a census to be taken every ten years. By the time of the 1880 census, information was accumulated on age, sex, and national origin. The data amassed took about seven years to process. </p>
<p class="calibre_1">Fearing that the 1890 census would take longer than a decade to process, the Census Office explored the possibility of automating the system and chose machinery developed by Herman <a shape="rect"></a>Hollerith (1860–1929), who had worked as a statistician for the 1880 census. </p>
<p class="calibre_1"><a shape="rect"></a>Hollerith's plan involved manila punch cards 6 ⅝ x 3 ¼ inches in size. (It's unlikely that Hollerith knew about Charles Babbage's use of cards to program his <a shape="rect"></a>Analytical Engine, but he was almost certainly familiar with the use of cards in the Jacquard loom.) The holes in these cards were organized into 24 columns of 12 positions each, for a total of 288 positions. These positions represented certain characteristics of a person being tallied in the census. The census taker indicated these characteristics by punching ¼-inch square holes into the appropriate positions on the card. </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00377.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">This book has probably so accustomed you to thinking in terms of binary codes that you might immediately assume that a card with 288 possible punches is capable of storing 288 bits of information. But the cards weren't used that way. </p>
<p class="calibre_1">For example, a census card used in a purely binary system would have one position for sex. It would be either punched for male or unpunched for female (or the other way around). But Hollerith's cards had two positions for sex. One position was punched for male, the other for female. Likewise, the census taker indicated a subject's age by making two punches. The first punch designated a five-year age range: 0 through 4, 5 through 9, 10 through 14, and so forth. The second punch was in one of five positions to indicate the precise age within that range. Coding the age required a total of 28 positions on the card. A pure binary system would require just 7 positions to code any age from 0 through 127. </p>
<p class="calibre_1">We should forgive Hollerith for not implementing a binary system for recording census information: Converting an age to binary numbers was a little too much to ask of the 1890 census takers. There's also a practical reason why a system of punched cards can't be entirely binary. A binary system would produce cases in which <span><em class="italic">all</em></span> the holes (or nearly all) were punched, rendering the card very fragile and structurally unsound. </p>
<p class="calibre_1">Census data is collected so that it can be counted, or <span><em class="italic">tabulated</em></span>. You want to know how many people live in each census district, of course, but it's also interesting to obtain information about the age distribution of the population. For this, Hollerith created a tabulating machine that combined hand operation and automation. An operator pressed a board containing 288 spring-loaded pins on each card. Pins corresponding to punched holes in the cards came into contact with a pool of mercury that completed an electrical circuit that triggered an electromagnet that incremented a decimal counter. </p>
<p class="calibre_1">Hollerith also used electromagnets in a machine that sorted cards. For example, you might want to accumulate separate age statistics for each occupation that you've tallied. You first need to sort the cards by occupation and then accumulate the age statistics separately for each. The sorting machine used the same hand press as the tabulator, but the sorter had electromagnets to open a hatch to one of 26 separate compartments. The operator dropped the card into the compartment and manually closed the hatch. </p>
<p class="calibre_1">This experiment in automating the 1890 census was a resounding success. All told, over 62 million cards were processed. They contained twice as much data as was accumulated in the 1880 census, and the data was processed in about one-third the time. Hollerith and his inventions became known around the world. In 1895, he even traveled to Moscow and succeeded in selling his equipment for use in the very first Russian census, which occurred in 1897. </p>
<p class="calibre_1">Herman Hollerith also set in motion a long trail of events. In 1896, he founded the <a shape="rect"></a>Tabulating Machine Company to lease and sell the punch-card equipment. By 1911, with the help of a couple of mergers, it had become the Computing-Tabulating-Recording Company, or C-T-R. By 1915, the president of C-T-R was Thomas J. <a shape="rect"></a>Watson (1874–1956), who in 1924 changed the name of the company to International Business Machines Corporation, or <a shape="rect"></a>IBM. </p>
<p class="calibre_1">By 1928, the original 1890 census cards had evolved into the famous "do not spindle, fold, or mutilate" IBM cards, with 80 columns and 12 rows. They remained in active use for over 50 years, and even in their later years were sometimes referred to as <span><em class="italic">Hollerith cards</em></span>. I'll describe the legacy of these cards more in Chapters <a shape="rect" href="index_split_023.html#filepos1367683">Chapter 20</a>, <a shape="rect" href="index_split_024.html#filepos1533684">Chapter 21</a>, and <a shape="rect" href="index_split_027.html#filepos1727052">Chapter 24</a>. </p>
<p class="calibre_1">Before we move on to the twentieth century, let's not leave the nineteenth century with too warped a view about that era. For obvious reasons, in this book I've been focusing most closely on inventions that are digital in nature. These include the <a shape="rect"></a>telegraph, <a shape="rect"></a>Braille, Babbage's engines, and the Hollerith card. When working with digital concepts and devices, you might find it easy to think that the whole world must be digital. But the nineteenth century is characterized more by discoveries and inventions that were decidedly <span><em class="italic">not</em></span> digital. Indeed, very little of the natural world that we experience through our senses is digital. It's instead mostly a continuum that can't be so easily quantified. </p>
<p class="calibre_1">Although Hollerith used relays in his card tabulators and sorters, people didn't really begin building computers using relays—<span><em class="italic">electromechanical</em></span> computers, as they were eventually called—until the mid 1930s. The relays used in these machines were generally not telegraph relays, but instead were relays developed for the telephone system to control the routing of calls. </p>
<p class="calibre_1">Those early relay computers were <span><em class="italic">not</em></span> like the relay computer that we built in the last chapter. (As we'll see, I based the design of that computer on microprocessors from the 1970s.) In particular, while it's obvious to us today that computers internally should use binary numbers, that wasn't always the case. </p>
<p class="calibre_1">Another difference between our relay computer and the early real ones is that nobody in the 1930s was crazy enough to construct 524,288 bits of <a shape="rect"></a>memory out of relays! The cost and space and power requirements would have made so much memory impossible. The scant memory available was used only for storing intermediate results. The programs themselves were on a physical medium such as a paper tape with punched holes. Indeed, our process of putting code and data into memory is a more modern concept. </p>
<p class="calibre_1">Chronologically, the first relay computer seems to have been constructed by Conrad <a shape="rect"></a>Zuse (1910–1995), who as an engineering student in 1935 began building a machine in his parents' apartment in Berlin. It used binary numbers but in the early versions used a mechanical memory scheme rather than relays. Zuse punched holes in old 35mm movie film to program his computers. </p>
<p class="calibre_1">In 1937, George <a shape="rect"></a>Stibitz (1904–1995) of <a shape="rect"></a>Bell Telephone Laboratories took home a couple of telephone relays and wired a 1-bit adder on his kitchen table that his wife later dubbed the K Machine (K for kitchen). This experimentation led to Bell Labs' Complex Number Computer in 1939. </p>
<p class="calibre_1">Meanwhile, Harvard graduate student Howard <a shape="rect"></a>Aiken (1900–1973) needed some way to do lots of repetitive calculations, and that led to a collaboration between Harvard and IBM that resulted in the Automated Sequence Controlled Calculator (ASCC) eventually known as the <a shape="rect"></a>Harvard Mark I, completed in 1943. This was the first digital computer that printed tables, thus finally realizing Charles Babbage's dream. The Mark II was the largest relay-based machine, using 13,000 relays. The Harvard Computation Laboratory headed by Aiken taught the first classes in computer science. </p>
<p class="calibre_1">Relays weren't perfect devices for constructing computers. Because they were mechanical and worked by bending pieces of metal, they could break after an extended workout. A relay could also fail because of a piece of dirt or paper stuck between the contacts. In one famous incident in 1947, a moth was extracted from a relay in the Harvard Mark II computer. Grace Murray <a shape="rect"></a>Hopper (1906–1992), who had joined Aiken's staff in 1944 and who would later become quite famous in the field of computer programming languages, taped the moth to the computer logbook with the note "first actual case of bug being found." </p>
<p class="calibre_1">A possible replacement for the relay is the vacuum tube, which was developed by John Ambrose <a shape="rect"></a>Fleming (1849–1945) and Lee de <a shape="rect"></a>Forest (1873–1961) in connection with radio. By the 1940s, <a shape="rect"></a>vacuum tubes had long been used to amplify telephones, and virtually every home had a console radio set filled with glowing tubes that amplified radio signals to make them audible. Vacuum tubes can also be wired—much like relays—into AND, OR, NAND, and NOR gates. </p>
<p class="calibre_1">It doesn't matter whether gates are built from relays or vacuum tubes. Gates can always be assembled into adders, selectors, decoders, flip-flops, and counters. Everything I explained about relay-based components in the preceding chapters remains valid when the relays are replaced by vacuum tubes. </p>
<p class="calibre_1">Vacuum tubes had their own problems, though. They were expensive, required a lot of electricity, and generated a lot of heat. The big problem, however, was that they eventually burned out. This was a fact of life that people lived with. Those who owned tube radios were accustomed to replacing tubes periodically. The telephone system was designed with a lot of redundancy, so the loss of a tube now and then was no big deal. (No one expects the telephone system to work flawlessly anyway.) When a tube burns out in a computer, however, it might not be immediately detected. Moreover, a computer uses so <span><em class="italic">many</em></span> vacuum tubes, that statistically they might be burning out every few minutes. </p>
<p class="calibre_1">The big advantage of using vacuum tubes over relays is that tubes can switch in about a millionth of a second—one <span><em class="italic">microsecond</em></span>. A vacuum tube changes state (switches on or off) a thousand times faster than a relay, which at its very best only manages to switch in about 1 millisecond, a thousandth of a second. Interestingly enough, the speed issue wasn't a major consideration in early computer development because overall computing speed was linked to the speed that the machine read the program from the paper or film tape. As long as computers were built in this way, it didn't matter how much faster vacuum tubes were than relays. </p>
<p class="calibre_1">But beginning in the early 1940s, vacuum tubes began supplanting relays in new computers. By 1945, the transition was complete. While relay machines were known as electromechanical computers, vacuum tubes were the basis of the first <span><em class="italic">electronic</em></span> computers. </p>
<p class="calibre_1">In Great Britain, the <a shape="rect"></a>Colossus computer (first operational in 1943) was dedicated to cracking the German "Enigma" code-making machine. Contributing to this project (and to some later British computer projects) was Alan M. <a shape="rect"></a>Turing (1912–1954), who is most famous these days for writing two influential papers. The first, published in 1937, pioneered the concept of "<a shape="rect"></a>computability," which is an analysis of what computers can and can't do. He conceived of an abstract model of a computer that's now known as the Turing Machine. The second famous paper Turing wrote was on the subject of artificial intelligence. He introduced a test for machine intelligence that's now known as the <a shape="rect"></a>Turing Test. </p>
<p class="calibre_1">At the Moore School of Electrical Engineering (University of Pennsylvania), J. Presper <a shape="rect"></a>Eckert (1919–1995) and John <a shape="rect"></a>Mauchly (1907–1980) designed the <a shape="rect"></a>ENIAC (Electronic Numerical Integrator and Computer). It used 18,000 vacuum tubes and was completed in late 1945. In sheer tonnage (about 30), the ENIAC was the largest computer that was ever (and probably will ever be) made. By 1977, you could buy a faster computer at <a shape="rect"></a>Radio Shack. Eckert and Mauchly's attempt to patent the computer was, however, thwarted by a competing claim of John V. <a shape="rect"></a>Atanasoff (1903–1995), who earlier designed an electronic computer that never worked quite right. </p>
<p class="calibre_1">The <a shape="rect"></a>ENIAC attracted the interest of mathematician John von <a shape="rect"></a>Neumann (1903–1957). Since 1930, the Hungarian-born von <a shape="rect"></a>Neumann (whose last name is pronounced <span><em class="italic">noy mahn</em></span>) had been living in the United States. A flamboyant man who had a reputation for doing complex arithmetic in his head, von Neumann was a mathematics professor at the Princeton Institute for Advanced Study, and he did research in everything from quantum mechanics to the application of game theory to economics. </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00378.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">John von Neumann helped design the successor to the ENIAC, the <a shape="rect"></a>EDVAC (Electronic Discrete Variable Automatic Computer). Particularly in the 1946 paper "<a shape="rect"></a>Preliminary Discussion of the Logical Design of an Electronic Computing Instrument," coauthored with Arthur W. <a shape="rect"></a>Burks and Herman H. <a shape="rect"></a>Goldstine, he described several features of a computer that made the EDVAC a considerable advance over the ENIAC. The designers of the EDVAC felt that the computer should use binary numbers internally. The ENIAC used decimal numbers. The computer should also have as much memory as possible, and this memory should be used for storing both program code and data as the program was being executed. (Again, this wasn't the case with the ENIAC. Programming the ENIAC was a matter of throwing switches and plugging in cables.) These instructions should be sequential in memory and addressed with a program counter but should also allow conditional jumps. This design came to be known as the <span><em class="italic">stored-program concept</em></span>. </p>
<p class="calibre_1">These design decisions were such an important evolutionary step that today we speak of <span><em class="italic">von Neumann architecture</em></span>. The computer that we built in the last chapter was a classic von Neumann machine. But with von Neumann architecture comes the <span><em class="italic">von Neumann bottleneck</em></span>. A von Neumann machine generally spends a significant amount of time just fetching instructions from memory in preparation for executing them. You'll recall that the final design of the <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> computer required that three-quarters of the time it spent on each instruction be involved in the instruction fetch.<a shape="rect"></a><a shape="rect"></a></p>
<p class="calibre_1">At the time of the EDVAC, it wasn't cost effective to build a lot of memory out of vacuum tubes. Some very odd solutions were proposed instead. One successful one was <span><em class="italic">mercury delay line memory</em></span>, which used 5-foot tubes of mercury. At one end of the tube, little pulses were sent into the mercury about 1 microsecond apart. These pulses took about a millisecond to reach the other end (where they were detected like sound waves and routed back to the beginning), and hence each tube of mercury could store about 1024 bits of information.<a shape="rect"></a></p>
<p class="calibre_1">It wasn't until the mid-1950s that <span><em class="italic">magnetic core memory</em></span> was developed. Such memory consisted of large arrays of little magnetized metal rings strung with wires. Each little ring could store a bit of information. Long after core memory had been replaced by other technologies, it was common to hear older programmers refer to the memory that the processor accessed as <span><em class="italic">core</em></span>.<a shape="rect"></a></p>
<p class="calibre_1">John von <a shape="rect"></a>Neumann wasn't the only person doing some major conceptual thinking about the nature of computers in the 1940s. </p>
<p class="calibre_1">Claude <a shape="rect"></a>Shannon (born 1916) was another influential thinker. In <a shape="rect" href="index_split_014.html#filepos501329">Chapter 11</a>, I discussed his 1938 master's thesis, which established the relationship between switches, relays, and <a shape="rect"></a>Boolean algebra. In 1948, while working for <a shape="rect"></a>Bell Telephone Laboratories, he published a paper in the <span><em class="italic">Bell System Technical Journal</em></span> entitled "A <a shape="rect"></a>Mathematical Theory of Communication" that not only introduced the word <span><em class="italic">bit</em></span> in print but established a field of study today known as <span><em class="italic">information theory</em></span>. Information theory is concerned with transmitting digital information in the presence of noise (which usually prevents all the information from getting through) and how to compensate for that. In 1949, he wrote the first article about programming a computer to play chess, and in 1952 he designed a mechanical <a shape="rect"></a>mouse controlled by relays that could learn its way around a maze. Shannon was also well known at Bell Labs for riding a unicycle and juggling simultaneously.<a shape="rect"></a></p>
<p class="calibre_1">Norbert <a shape="rect"></a>Wiener (1894–1964), who earned his Ph.D. in mathematics from Harvard at the age of 18, is most famous for his book <span><em class="italic">Cybernetics, or Control and Communication in the Animal and Machine</em></span> (1948). He coined the word <span><em class="italic">cybernetics</em></span> (derived from the Greek for <span><em class="italic">steersman</em></span>) to identify a theory that related biological processes in humans and animals to the mechanics of computers and robots. In popular culture, the ubiquitous <span><em class="italic">cyber</em></span>-prefix now denotes anything related to the computer. Most notably, the interconnection of millions of computers through the Internet is known as <span><em class="italic">cyberspace</em></span>, a word coined by <span><em class="italic">cyberpunk</em></span> sciencefiction novelist William <a shape="rect"></a>Gibson in his 1984 novel <span><em class="italic">Neuromancer</em></span>.<a shape="rect"></a><a shape="rect"></a></p>
<p class="calibre_1">In 1948, the <a shape="rect"></a>Eckert-Mauchly <a shape="rect"></a>Computer Corporation (later part of <a shape="rect"></a>Remington Rand) began work on what would become the first commercially available computer—the Universal Automatic Computer, or <a shape="rect"></a>UNIVAC. It was completed in 1951, and the first one was delivered to the Bureau of the Census. The UNIVAC made its prime-time network debut on CBS, when it was used to predict results of the 1952 presidential election. Walter Cronkite referred to it as an "electronic brain." Also in 1952, <a shape="rect"></a>IBM announced the company's first commercial computer system, the 701. </p>
<p class="calibre_1">And thus began a long history of corporate and governmental computing. However interesting that history might be, we're going to pursue another historical track—a track that shrank the cost and size of computers and brought them into the home, and which began with an almost unnoticed electronics breakthrough in 1947. </p>
<p class="calibre_1">Bell Telephone Laboratories was for many years a place where smart people could work on just about anything that interested them. Some of them, fortunately, were interested in computers. I've already mentioned George <a shape="rect"></a>Stibitz and Claude Shannon, both of whom made significant contributions to early computing while working at Bell Labs. Later on, in the 1970s, Bell Labs was the birthplace of the influential computer operating system named <a shape="rect"></a>Unix and a programming language named C, which I'll describe in upcoming chapters. </p>
<p class="calibre_1">Bell Labs came about when American Telephone and Telegraph officially separated their scientific and technical research divisions from the rest of their business, creating the subsidiary on January 1, 1925. The primary purpose of Bell Labs was to develop technologies for improving the telephone system. That mandate was fortunately vague enough to encompass all sorts of things, but one obvious perennial goal within the telephone system was the undistorted amplification of voice signals transmitted over wires. </p>
<p class="calibre_1">Since 1912, the Bell System had worked with vacuum tube amplification, and a considerable amount of research and engineering went into improving <a shape="rect"></a>vacuum tubes for use by the telephone system. Despite this work, vacuum tubes still left much to be desired. Tubes were large, consumed a lot of power, and eventually burned out. But they were the only game in town. </p>
<p class="calibre_1">All that changed December 16, 1947, when two physicists at Bell Labs named John <a shape="rect"></a>Bardeen (1908–1991) and Walter <a shape="rect"></a>Brattain (1902–1987) wired a different type of amplifier. This new amplifier was constructed from a slab of germanium—an element known as a <span><em class="italic">semiconductor</em></span>—and a strip of gold foil. They demonstrated it to their boss, William <a shape="rect"></a>Shockley (1910–1989), a week later. It was the first <span><em class="italic">transistor</em></span>, a device that some people have called the most important invention of the twentieth century. </p>
<p class="calibre_1">The transistor didn't come out of the blue. Eight years earlier, on December 29, 1939, Shockley had written in his notebook, "It has today occurred to me that an amplifier using semiconductors rather than vacuum is in principle possible." And after that first transistor was demonstrated, many years followed in perfecting it. It wasn't until 1956 that Shockley, Bardeen, and Brattain were awarded the <a shape="rect"></a>Nobel Prize in physics "for their researches on semiconductors and their discovery of the transistor effect." </p>
<p class="calibre_1">Earlier in this book, I talked about conductors and insulators. Conductors are so called because they're very conducive to the passage of electricity. Copper, silver, and gold are the best conductors, and it's no coincidence that all three are found in the same column of the periodic table of the elements. </p>
<p class="calibre_1">As you'll recall, the electrons in an atom are distributed in shells that surround the nucleus of the atom. What characterizes these three conductors is a lone electron in the outermost shell. This electron can be easily dislodged from the rest of the atom and hence is free to move as electrical current. The opposites of conductors are insulators—like rubber and plastic—that barely conduct electricity at all. </p>
<p class="calibre_1">The elements germanium and silicon (as well as some compounds) are called <span><em class="italic">semiconductors</em></span>, not because they conduct half as well as conductors, but because their conductance can be manipulated in various ways. Semiconductors have four electrons in the outermost shell, which is half the maximum number the outer shell can have. In a pure semiconductor, the atoms form very stable bonds with each other and have a crystalline structure similar to the diamond. Such semiconductors aren't good conductors. </p>
<p class="calibre_1">But semiconductors can be <span><em class="italic">doped</em></span>, which means that they're combined with certain impurities. One type of impurity adds extra electrons to those needed for the bond between the atoms. These are called <span><em class="italic">N-type semiconductors</em></span> (N for <span><em class="italic">negative</em></span>). Another type of impurity results in a <span><em class="italic">P-type semiconductor</em></span>. </p>
<p class="calibre_1">Semiconductors can be made into amplifiers by sandwiching a P-type semiconductor between two N-type semiconductors. This is known as an <a shape="rect"></a>NPN transistor, and the three pieces are known as the <span><em class="italic">collector</em></span>, the <span><em class="italic">base</em></span>, and the <span><em class="italic">emitter</em></span>. </p>
<p class="calibre_1">Here's a schematic diagram of an NPN transistor:</p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00379.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">A small voltage on the base can control a much larger voltage passing from the collector to the emitter. If there's no voltage on the base, it effectively turns off the transistor. </p>
<p class="calibre_1">Transistors are usually packaged in little metal cans about a quarter-inch in diameter with three wires poking out:</p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00380.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">The transistor inaugurated <span><em class="italic">solid-state</em></span> electronics, which means that transistors don't require vacuums and are built from solids, specifically semiconductors and most commonly (these days) silicon. Besides being much smaller than vacuum tubes, transistors require much less power, generate much less heat, and last longer. Carrying around a tube radio in your pocket was inconceivable. But a transistor radio could be powered by a small battery, and unlike tubes, it wouldn't get hot. Carrying a transistor radio in your pocket became possible for some lucky people opening presents on Christmas morning in 1954. Those first pocket radios used transistors made by Texas Instruments, an important company of the semiconductor revolution.<a shape="rect"></a></p>
<p class="calibre_1">The <span><em class="italic">first</em></span> commercial application of the transistor was, however, a hearing aid. In commemorating the heritage of Alexander Graham <a shape="rect"></a>Bell in his lifelong work with deaf people, <a shape="rect"></a>AT&amp;T allowed hearing aid manufacturers to use transistor technology without paying any royalties. The first transistor television debuted in 1960, and today tube appliances have almost disappeared. (Not entirely, however. Some audiophiles and electric guitarists continue to prefer the sound of tube amplifiers to their transistor counterparts.) </p>
<p class="calibre_1">In 1956, Shockley left Bell Labs to form Shockley Semiconductor Laboratories. He moved to Palo Alto, California, where he had grown up. His was the first such company to locate in that area. In time, other semiconductor and computer companies set up business there, and the area south of San Francisco is now informally known as Silicon Valley. </p>
<p class="calibre_1">Vacuum tubes were originally developed for amplification, but they could also be used for switches in logic gates. The same goes for the transistor. On the next page, you'll see a transistor-based AND gate structured much like the relay version. Only when both the A input is 1 <a shape="rect"></a>and the B input is 1 will both transistors conduct current and hence make the output 1. The resistor prevents a short circuit when this happens. </p>
<p class="calibre_1">Wiring two transistors as you see below in the <a shape="rect"></a>diagram on the right creates an OR gate. In the AND gate, the emitter of the top transistor is connected to the collector of the bottom transistor. In the OR gate, the collectors of both transistors are connected to the voltage supply. The emitters are connected together. </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00381.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">So everything we learned about constructing logic gates and other components from relays is valid for transistors. Relays, tubes, and transistors were all initially developed primarily for purposes of amplification but can be connected in similar ways to make logic gates out of which computers can be built. The first transistor computers were built in 1956, and within a few years tubes had been abandoned for the design of new computers. </p>
<p class="calibre_1">Here's a question: Transistors certainly make computers more reliable, smaller, and less power hungry. But do transistors make computers any simpler to <span><em class="italic">construct</em></span>? </p>
<p class="calibre_1">Not really. The transistor lets you fit more logic gates in a smaller space, of course, but you still have to worry about all the <span><em class="italic">interconnections</em></span> of these components. It's just as difficult wiring transistors to make logic gates as it is wiring relays and <a shape="rect"></a>vacuum tubes. In some ways, it's even more difficult because the transistors are smaller and less easy to hold. If you wanted to build the <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> computer and the 64-KB RAM array out of transistors, a good part of the design work would be devoted to inventing some kind of structure in which to hold all the components. Most of your physical labor would be the tedious wiring of millions of interconnections among millions of transistors. </p>
<p class="calibre_1">As we've discovered, however, there are certain combinations of transistors that show up repeatedly. Pairs of transistors are almost always wired as gates. Gates are often wired into flip-flops or adders or selectors or decoders. Flip-flops are combined into multibit latches or <a shape="rect"></a>RAM arrays. Assembling a computer would be much easier if the transistors were prewired in common configurations. </p>
<p class="calibre_1">This idea seems to have been proposed first by British physicist Geoffrey <a shape="rect"></a>Dummer (born 1909) in a speech in May 1952. "I would like to take a peep into the future," he said. </p>
<div class="calibre_"><div class="calibre11">
<blockquote class="calibre3"><div class="calibre12">
<div class="calibre_">With the advent of the transistor and the work in semiconductors generally, it seems now possible to envisage electronic equipment in a solid block with no connecting wires. The block may consist of layers of insulating, conducting, rectifying and amplifying materials, the electrical functions being connected directly by cutting out areas of the various layers. </div>
</div></blockquote>
</div></div>
<p class="calibre_1">A working product, however, would have to wait a few years.</p>
<p class="calibre_1">Without knowing about the Dummer prediction, in July 1958 it occurred to Jack <a shape="rect"></a>Kilby (born 1923) of <a shape="rect"></a>Texas Instruments that multiple transistors as well as resistors and other electrical components could be made from a single piece of silicon. Six months later, in January 1959, basically the same idea occurred to Robert <a shape="rect"></a>Noyce (1927–1990). Noyce had originally worked for Shockley Semiconductor Laboratories, but in 1957 he and seven other scientists had left and started <a shape="rect"></a>Fairchild Semiconductor Corporation. </p>
<p class="calibre_1">In the history of technology, simultaneous invention is more common than one might suspect. Although Kilby had invented the device six months before Noyce, and Texas Instruments had applied for a patent before Fairchild, Noyce was issued a patent first. Legal battles ensued, and only after a decade were they finally settled to everyone's satisfaction. Although they never worked together, Kilby and Noyce are today regarded as the coinventors of the <span><em class="italic">integrated circuit</em></span>, or <span><em class="italic">IC</em></span>, commonly called the <span><em class="italic">chip</em></span>. </p>
<p class="calibre_1">Integrated circuits are manufactured through a complex process that involves layering thin wafers of silicon that are precisely doped and etched in different areas to form microscopic components. Although it's expensive to develop a new integrated circuit, they benefit from mass production—the more you make, the cheaper they become. </p>
<p class="calibre_1">The actual silicon chip is thin and delicate, so it must be securely packaged, both to protect the chip and to provide some way for the components in the chip to be connected to other chips. Integrated circuits are packaged in a couple of different ways, but the most common is the rectangular plastic <span><em class="italic">dual inline package</em></span> (or DIP), with 14, 16, or as many as 40 pins protruding from the side: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00382.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">This is a 16-pin chip. If you hold the chip so the little indentation is at the left (as shown), the pins are numbered 1 through 16 beginning at the lower left and circling around the right side to end with pin 16 at the upper left. The pins on each side are exactly <span><a shape="rect"></a><img alt="" src="images/00383.jpg" class="calibre10"/></span> inch apart. </p>
<p class="calibre_1">Throughout the 1960s, the space program and the arms race fueled the early integrated circuits market. On the civilian side, the first commercial product that contained an integrated circuit was a hearing aid sold by <a shape="rect"></a>Zenith in 1964. In 1971, <a shape="rect"></a>Texas Instruments began selling the first pocket calculator, and Pulsar the first digital watch. (Obviously the IC in a digital watch is packaged much differently from the example just shown.) Many other products that incorporated integrated circuits in their design followed. </p>
<p class="calibre_1">In 1965, Gordon E. <a shape="rect"></a>Moore (then at Fairchild and later a cofounder of Intel Corporation) noticed that technology was improving in such a way that the number of transistors that could fit on a single chip had doubled every year since 1959. He predicted that this trend would continue. The actual trend was a little slower, so <a shape="rect"></a>Moore's Law (as it was eventually called) was modified to predict a doubling of transistors on a chip every 18 months. This is still an astonishingly fast rate of progress and reveals why home computers always seem to become outdated in just a few short years. Some people believe that Moore's Law will continue to be accurate until about 2015. </p>
<p class="calibre_1">In the early days, people used to speak of <span><em class="italic">small-scale integration</em></span>, or SSI, to refer to a chip that had fewer than 10 logic gates; <span><em class="italic">medium-scale integration</em></span>, or MSI (10 to 100 gates); and <span><em class="italic">large-scale integration</em></span>, or LSI (100 to 5000). Then the terms ascended to <span><em class="italic">very-large-scale integration</em></span>, or VLSI (5000 to 50,000); <span><em class="italic">super-large-scale integration</em></span>, or SLSI (50,000 to 100,000); and <span><em class="italic">ultra-large-scale integration</em></span>, (more than 100,000 gates). </p>
<p class="calibre_1">For the remainder of this chapter and the next, I want to pause our time machine in the mid-1970s, an ancient age before the first <span><em class="italic">Star Wars</em></span> movie was released and with VLSI just on the horizon. At that time, several different technologies were used to fabricate the components that make up integrated circuits. Each of these technologies is sometimes called a <span><em class="italic">family</em></span> of ICs. By the mid-1970s, two families were prevalent: TTL (pronounced <span><em class="italic">tee tee ell</em></span>) and <a shape="rect"></a>CMOS (<span><em class="italic">see moss</em></span>). </p>
<p class="calibre_1">TTL stands for <span><em class="italic">transistor-transistor logic</em></span>. If in the mid-1970s you were a digital design engineer (which meant that you designed larger circuits from ICs), a 1 ¼-inch-thick book first published in 1973 by Texas Instruments called <span><em class="italic">The TTL Data Book for Design Engineers</em></span> would be a permanent fixture on your desk. This is a complete reference to the 7400 (<span><em class="italic">seventy-four hundred</em></span>) series of TTL integrated circuits sold by Texas Instruments and several other companies, so called because each IC in this family is identified by a number beginning with the digits 74. </p>
<p class="calibre_1">Every integrated circuit in the 7400 series consists of logic gates that are prewired in a particular configuration. Some chips provide simple prewired gates that you can use to create larger components; other chips provide common components such as flip-flops, adders, selectors, and decoders. </p>
<p class="calibre_1">The first IC in the 7400 series is number 7400 itself, which is described in the <span><em class="italic">TTL Data Book</em></span> as "Quadruple 2-Input Positive-NAND Gates." What this means is that this particular integrated circuit contains four 2-input NAND gates. They're called <span><em class="italic">positive</em></span> NAND gates because a voltage corresponds to 1 and no voltage corresponds to 0. This is a 14-pin chip, and a little diagram in the data book shows how the pins correspond to the inputs and outputs: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00384.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">This diagram is a top view of the chip (pins on the bottom) with the little indentation (shown on page 250) at the left.</p>
<p class="calibre_1">Pin 14 is labeled V<sub class="calibre25">CC</sub> and is equivalent to the V symbol that I've been using to indicate a voltage. (By convention, any double letter subscript on a capital V indicates a power supply. The C in this subscript refers to the <span><em class="italic">collector</em></span> input of a transistor, which is internally where the voltage supply is connected.) Pin 7 is labeled GND for <span><em class="italic">ground</em></span>. Every integrated circuit that you use in a particular circuit must be connected to a power supply and a common ground. </p>
<p class="calibre_1">For 7400 series TTL, V<sub class="calibre25">CC</sub> must be between 4.75 and 5.25 volts. Another way of saying this is that the power supply voltage must be 5 volts plus or minus 5 percent. If the power supply is below 4.75 volts, the chip might not work. If it's higher than 5.25, the chip could be damaged. You generally can't use batteries with TTL; even if you were to find a 5-volt battery, the voltage wouldn't be exact enough to be adequate for these chips. TTL usually requires a power supply that you plug into the wall. </p>
<p class="calibre_1">Each of the four NAND gates in the 7400 chip has two inputs and one output. They work independently of each other. In past chapters, we've been differentiating between inputs being either 1 (which is a voltage) or 0 (which is no voltage). In reality, an input to one of these NAND gates can range anywhere from 0 volts (ground) to 5 volts (V<sub class="calibre25">CC</sub>). In TTL, anything between 0 volts and 0.8 volt is considered to be a logical 0, and anything between 2 volts and 5 volts is considered to be a logical 1. Inputs between 0.8 volt and 2 volts should be avoided. </p>
<p class="calibre_1">The output of a TTL gate is typically about 0.2 volt for a logical 0 and 3.4 volts for a logical 1. Because these voltages can vary somewhat, inputs and outputs to integrated circuits are sometimes referred to as <span><em class="italic">low</em></span> and <span><em class="italic">high</em></span> rather than 0 and 1. Moreover, sometimes a low voltage can mean a logical 1 and a high voltage can mean a logical 0. This configuration is referred to as <span><em class="italic">negative logic</em></span>. When the 7400 chip is referred to as "Quadruple 2-Input Positive-NAND Gates," the word <span><em class="italic">positive</em></span> means positive logic is assumed. </p>
<p class="calibre_1">If the output of a TTL gate is typically 0.2 volt for a logical 0 and 3.4 volts for a logical 1, these outputs are safely within the input ranges, which are between 0 and 0.8 volt for a logical 0 and between 2 and 5 volts for a logical 1. This is how TTL is insulated against <span><em class="italic">noise</em></span>. A 1 output can lose about 1.4 volts and still be high enough to qualify as a 1 input. A 0 output can gain 0.6 volt and still be low enough to qualify as a 0 input. </p>
<p class="calibre_1">Probably the most important fact to know about a particular integrated circuit is the <span><em class="italic">propagation time</em></span>. That's the time it takes for a change in the inputs to be reflected in the output. </p>
<p class="calibre_1">Propagation times for chips are generally measured in <span><em class="italic">nanoseconds</em></span>, abbreviated nsec. A nanosecond is a <span><em class="italic">very</em></span> short period of time. One thousandth of a second is a millisecond. One millionth of a second is a microsecond. One billionth of a second is a nanosecond. The propagation time for the NAND gates in the 7400 chip is guaranteed to be less than 22 nanoseconds. That's 0.000000022 seconds, or 22 billionths of a second.<a shape="rect"></a></p>
<p class="calibre_1">If you can't get the feel of a nanosecond, you're not alone. Nobody on this planet has anything but an intellectual appreciation of the nanosecond. Nanoseconds are much shorter than anything in human experience, so they'll forever remain incomprehensible. Every explanation makes the nanosecond more elusive. For example, I can say that if you're holding this book 1 foot away from your face, a nanosecond is the time it takes the light to travel from the page to your eyes. But do you really have a better feel for the nanosecond now? </p>
<p class="calibre_1">Yet the nanosecond is what makes computers possible. As we saw in <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a>, a computer processor does moronically simple things—it moves a byte from memory to register, adds a byte to another byte, moves the result back to memory. The only reason anything substantial gets completed (not in the <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> computer but in real ones) is that these operations occur very quickly. To quote Robert <a shape="rect"></a>Noyce, "After you become reconciled to the nanosecond, computer operations are conceptually fairly simple." </p>
<p class="calibre_1">Let's continue perusing the <span><em class="italic">TTL Data Book for Design Engineers</em></span>. You will see a lot of familiar little items in this book. The 7402 chip contains four 2-input NOR gates, the 7404 has six inverters, the 7408 has four 2-input AND gates, the 7432 has four 2-input OR gates, and the 7430 has an 8-input NAND gate: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00385.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">The abbreviation <span><em class="italic">NC</em></span> means <span><em class="italic">no connection</em></span>. </p>
<p class="calibre_1">The 7474 chip is another that will sound very familiar. It's a "Dual D-Type Positive-Edge-Triggered Flip-Flop with Preset and Clear" and is diagrammed like this: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00386.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">The <span><em class="italic">TTL Data Book</em></span> even includes a logic diagram for each flip-flop in this chip: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00387.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">You'll recognize this as being similar to the diagram at the end of <a shape="rect" href="index_split_017.html#filepos669318">Chapter 14</a>, except that I used NOR gates. The logic table in the <span><em class="italic">TTL Data Book</em></span> is a little different as well: </p>
<div class="calibre_">
<table border="1" class="calibre13"><div class="calibre11">
<colgroup span="1" class="calibre14">
<col span="1" class="calibre15"/>
<col span="1" class="calibre15"/>
<col span="1" class="calibre15"/>
<col span="1" class="calibre15"/>
<col span="1" class="calibre15"/>
<col span="1" class="calibre15"/>
</colgroup>
<thead class="calibre16">
<tr class="calibre17">
<th colspan="4" rowspan="1" class="calibre18"><span class="calibre12"><span class="calibre9">Inputs</span></span></th>
<th colspan="2" rowspan="1" class="calibre18"><span class="calibre12"><span class="calibre9">Outputs</span></span></th>
</tr>
</thead>
<tbody class="calibre19">
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><strong class="calibre9">Pre</strong></span></p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><strong class="calibre9">Clr</strong></span></p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><strong class="calibre9">Clk</strong></span></p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><strong class="calibre9">D</strong></span></p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><strong class="calibre9">Q</strong></span></p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><a shape="rect"></a><img alt="" src="images/00258.jpg" class="calibre10"/></span></p>
</div></td>
</tr>
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
</tr>
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
</tr>
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H*</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H*</p>
</div></td>
</tr>
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">↑</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
</tr>
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">↑</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
</tr>
<tr class="calibre17">
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">H</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">L</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">X</p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1">Q<sub class="calibre28">0</sub></p>
</div></td>
<td rowspan="1" colspan="1" border="1" class="calibre20"><div class="calibre12">
<p class="calibre_1"><span><a shape="rect"></a><img alt="" src="images/00258.jpg" class="calibre10"/></span><sub class="calibre28">0</sub></p>
</div></td>
</tr>
</tbody>
</div></table>
</div>
<p class="calibre_1">In this table, the <span><em class="italic">H</em></span> stands for <span><em class="italic">High</em></span> and the <span><em class="italic">L</em></span> stands for <span><em class="italic">Low</em></span>. You can think of these as 1 and 0 if you wish. In my flip-flop, the Preset and Clear inputs were normally 0; here they're normally 1. </p>
<p class="calibre_1">Moving right along in the <span><em class="italic">TTL Data Book</em></span>, you'll discover that the 7483 chip is a 4-Bit Binary Full Adder, 74151 is a 8-Line-To-1-Line Data Selector, the 74154 is a 4-line-To-16-Line Decoder, 74161 is a Synchronous 4-Bit Binary Counter, and 74175 is a Quadruple D-Type Flip-Flop with Clear. You can use two of these chips for making an 8-bit latch. </p>
<p class="calibre_1">So now you know how I came up with all the various components I've been using since <a shape="rect" href="index_split_014.html#filepos501329">Chapter 11</a>. I stole them from the <span><em class="italic">TTL Data Book for Design Engineers</em></span>. </p>
<p class="calibre_1">As a digital design engineer, you would spend long hours going through the <span><em class="italic">TTL Data Book</em></span> familiarizing yourself with the types of TTL chips that were available. Once you knew all your tools, you could actually build the computer I showed in <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> out of TTL chips. Wiring the chips together is a lot easier than wiring individual transistors together. But you might want to consider <span><em class="italic">not</em></span> using TTL to make the 64-KB <a shape="rect"></a>RAM array. In the 1973 <span><em class="italic">TTL Data Book</em></span>, the heftiest RAM chip listed is a mere 256 x 1 bits. You'd need 2048 of these chips to make 64 KB! TTL was never the best technology for memory. I'll have more to say about memory in <a shape="rect" href="index_split_024.html#filepos1533684">Chapter 21</a>. </p>
<p class="calibre_1">You'd probably want to use a better oscillator as well. While you can certainly connect the output of a TTL inverter to the input, it's better to have an oscillator with a more predictable frequency. Such an oscillator can be constructed fairly easily using a quartz crystal that comes in a little flat can with two wires sticking out. These crystals vibrate at very specific frequencies, usually at least a million cycles per second. A million cycles per second is called a <span><em class="italic">megahertz</em></span> and abbreviated MHz. If the <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> computer were constructed out of TTL, it would probably run fine with a clock frequency of 10 MHz. Each instruction would execute in 400 nanoseconds. This, of course, is much faster than anything we conceived when we were working with relays. </p>
<p class="calibre_1">The other popular chip family was (and still is) <a shape="rect"></a>CMOS, which stands for <span><em class="italic">complementary metal-oxide semiconductor</em></span>. If you were a hobbyist designing circuits from CMOS ICs in the mid-1970s, you might use as a reference source a book published by National Semiconductor and available at your local Radio Shack entitled <span><em class="italic">CMOS Databook</em></span>. This book contains information about the 4000 (<span><em class="italic">four thousand</em></span>) series of CMOS ICs. </p>
<p class="calibre_1">The power supply requirement for TTL is 4.75 to 5.25 volts. For CMOS, it's anything from 3 volts to 18 volts. That's quite a leeway! Moreover, CMOS requires much less power than TTL, which makes it feasible to run small CMOS circuits from batteries. The drawback of CMOS is lack of speed. For example, the CMOS 4008 4-bit full adder running at 5 volts is only guaranteed to have a propagation time of 750 nanoseconds. It gets faster as the power supply gets higher—250 nsec at 10 volts and 190 nsec at 15 volts. But the CMOS device doesn't come close to the TTL 4-bit adder, which has a propagation time of 24 nsec. (Twenty-five years ago, the trade-off between the speed of TTL and the low power requirements of CMOS was fairly clear cut. Today there are low-power versions of TTL and high-speed versions of CMOS.) </p>
<p class="calibre_1">On the practical side, you would probably begin wiring chips together on a plastic <span><em class="italic">breadboard</em></span>: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00388.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">Each short row of 5 holes is electrically connected underneath the plastic base. You insert chips into the breadboard so that a chip straddles the long central groove and the pins go into the holes on either side of the groove. Each pin of the IC is then electrically connected to 4 other holes. You connect the chips with pieces of wires pushed into the other holes. </p>
<p class="calibre_1">You can wire chips together more permanently using a technique called <span><em class="italic">wire-wrapping</em></span>. Each chip is inserted into a socket that has long square posts: </p>
<div class="calibre_">
<div class="calibre_"><a shape="rect"></a><img alt="image with no caption" src="images/00389.jpg" class="calibre10"/></div>
</div>
<p class="calibre_1">Each post corresponds to a pin of the chip. The sockets themselves are inserted into thin perforated boards. From the other side of the board, you use a special wire-wrap gun to tightly wrap thin pieces of insulated wire around the post. The square edges of the post break through the insulation and make an electrical connection with the wire. </p>
<p class="calibre_1">If you were actually manufacturing a particular circuit using ICs, you'd probably use a <span><em class="italic">printed circuit</em></span> board. Back in the old days, this was something a hobbyist could do. Such a board has holes and is covered by a thin layer of copper foil. Basically, you cover all the areas of copper you want to preserve with an acid resistant and use acid to etch away the rest. You can then solder IC sockets (or the ICs themselves) directly to the copper on the board. But because of the very many interconnections among ICs, a single area of copper foil is usually inadequate. Commercially manufactured printed circuit boards have multiple layers of interconnections. </p>
<p class="calibre_1">By the early 1970s, it became possible to use ICs to create an entire computer processor on a single circuit board. It was really only a matter of time before somebody put the whole processor on a single chip. Although <a shape="rect"></a>Texas Instruments filed a patent for a single-chip computer in 1971, the honor of actually making one belongs to Intel, a company started in 1968 by former Fairchild employees Robert <a shape="rect"></a>Noyce and Gordon <a shape="rect"></a>Moore. Intel's first major product was, in 1970, a memory chip that stored 1024 bits, which was the greatest number of bits on a chip at that time. </p>
<p class="calibre_1">Intel was in the process of designing chips for a programmable calculator to be manufactured by the Japanese company <a shape="rect"></a>Busicom when they decided to take a different approach. As Intel engineer Ted <a shape="rect"></a>Hoff put it, "Instead of making their device act like a calculator with some programming abilities, I wanted to make it function as a general-purpose computer programmed to be a calculator." This led to the Intel 4004 (pronounced <span><em class="italic">forty oh four</em></span>), the first "computer on a chip," or <span><em class="italic">microprocessor</em></span>. The 4004 became available in November 1971 and contained 2300 transistors. (By Moore's Law, <a shape="rect"></a>microprocessors made 18 years later should contain about 4000 times as many transistors, or about 10 million. That's a fairly accurate prediction.) </p>
<p class="calibre_1">Having told you the number of its transistors, I'll now describe three other important characteristics of the 4004. These three measures are often used as standards for comparison among microprocessors since the 4004. </p>
<p class="calibre_1">First, the 4004 was a <span><em class="italic">4-bit</em></span> microprocessor. This means that the data paths in the processor were only 4 bits wide. When adding or subtracting numbers, it handled only 4 bits at a shot. In contrast, the computer developed in <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> has 8-bit data paths and is thus an 8-bit processor. As we'll soon see, 4-bit microprocessors were surpassed very quickly by 8-bit microprocessors. No one stopped there. In the late 1970s, 16-bit microprocessors became available. When you think back to <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> and recall the several instruction codes necessary to add two 16-bit numbers on an 8-bit processor, you'll appreciate the advantage that a 16-bit processor gives you. In the mid-1980s, 32-bit microprocessors were introduced and have remained the standard for home computers since then. </p>
<p class="calibre_1">Second, the 4004 had a maximum <span><em class="italic">clock speed</em></span> of 108,000 cycles per second, or 108 <span><em class="italic">kilohertz</em></span> (KHz). Clock speed is the maximum <a shape="rect"></a>speed of an oscillator that you can connect to the microprocessor to make it go. Any faster and it might not work right. By 1999, microprocessors intended for home computers had hit the 500-megahertz point—about 5000 times faster than the 4004. </p>
<p class="calibre_1">Third, the <span><em class="italic">addressable memory</em></span> of the 4004 was 640 bytes. This seems like an absurdly low amount; yet it was in line with the capacity of memory chips available at the time. As you'll see in the next chapter, within a couple of years microprocessors could address 64 KB of memory, which is the capability of the <a shape="rect" href="index_split_020.html#filepos950185">Chapter 17</a> machine. Intel microprocessors in 1999 can address 64 terabytes of memory, although that's overkill considering that most people have fewer than 256 megabytes of RAM in their home computers. </p>
<p class="calibre_1">These three numbers don't affect the <span><em class="italic">capability</em></span> of a computer. A 4-bit processor can add 32-bit numbers, for example, simply by doing it in 4-bit chunks. In one sense, all digital computers are the same. If the hardware of one processor can do something another can't, the other processor can do it in software; they all end up doing the same thing. This is one of the implications of Alan Turing's 1937 paper on <a shape="rect"></a>computability. </p>
<p class="calibre_1">Where processors ultimately <span><em class="italic">do</em></span> differ, however, is in <span><em class="italic">speed</em></span>. And speed is a big reason why we're using computers to begin with. </p>
<p class="calibre_1">The maximum clock speed is an obvious influence on the overall speed of a processor. That clock speed determines how fast each instruction is being executed. The processor data width affects speed as well. Although a 4-bit processor can add 32-bit numbers, it can't do it nearly as fast as a 32-bit processor. What might be confusing, however, is the effect on speed of the maximum amount of memory that a processor can address. At first, addressable memory seems to have nothing to do with speed and instead reflects a limitation on the processor's ability to perform certain functions that might require a lot of memory. But a processor can always get around the memory limitation by using some memory addresses to control some other medium for saving and retrieving information. (For example, suppose every byte written to a particular memory address is actually punched on a paper tape, and every byte read from that address is read from the tape.) What happens, however, is that this process slows down the whole computer. The issue again is speed. </p>
<p class="calibre_1">Of course, these three numbers indicate only roughly how fast the microprocessor operates. These numbers tell you nothing about the internal architecture of the microprocessor or about the efficiency and capability of the machine-code instructions. As processors have become more sophisticated, many common tasks previously done in software have been built into the processor. We'll see examples of this trend in the chapters ahead. </p>
<p class="calibre_1">Even though all digital computers have the same capabilities, even though they can do nothing beyond the primitive computing machine devised by Alan Turing, the speed of a processor <span><em class="italic">of course</em></span> ultimately affects the over-all usefulness of a computer system. Any computer that's slower than the human brain in performing a set of calculations is useless, for example. And we can hardly expect to watch a movie on our modern computer screens if the processor needs a minute to draw a single frame. </p>
<p class="calibre_1">But back to the mid-1970s. Despite the limitations of the 4004, it was a start. By April 1972, Intel had released the 8008—an 8-bit microprocessor running at 200 kHz that could address 16 KB of memory. (See how easy it is to sum up a processor with just three numbers?) And then, in a five-month period in 1974, both Intel and Motorola came out with microprocessors that were intended to improve on the 8008. These two chips changed the world. </p>
</div>  <div class="mbp_pagebreak" id="calibre_pb_62"></div>
</body></html>
